{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes made Team\n",
    "- `glove` was not available and so needed to install `mittens` instead\n",
    "  - `from mittens import GloVe as glove`\n",
    "- The calls to access the `Word2vec` and `FastText` models needed to be updated\n",
    "  - `w2vec[...]` changed to `w2vec.wv[...]` and `fasttext[...]` changed to `fasttext.wv[...]`\n",
    "- In Python 3 you can no longer convert from `map` to `np.array`\n",
    "  - So, `t = np.asarray(map(mean, zip(*avg)))` changed to `t = np.asarray(list(map(mean, zip(*avg))))`\n",
    "- Accessing `FastText` elements raised an `IndexError`\n",
    "  - These calls are now wraped in a `try` block\n",
    "- Added in two new embedding techniques `ClinicalBERT` and `BlueBERT`\n",
    "- Varaibles saved into `new_ner_XXXX_limited_dict.pkl` were not being calculated correctly. \n",
    "  - Only common keys should be saved in each so each should have the same length prior to saving.\n",
    "- Changes to newlines / spacings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ayushghosh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify what to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_word2vec     = True\n",
    "run_fastText     = True\n",
    "run_combined     = True\n",
    "run_blueBERT     = False\n",
    "run_clinicalBERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "\n",
    "if run_word2vec: w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "if run_fastText: fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vec starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 22446/22446 [00:12<00:00, 1746.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vec finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if run_word2vec:\n",
    "    print(\"w2vec starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_word2vec = {}\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(w2vec.wv[i[0]])\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            continue\n",
    "                        avg = np.asarray(avg)\n",
    "                        t = np.asarray(list(map(mean, zip(*avg))))\n",
    "                        patient_temp.append(t)\n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_word2vec[k] = patient_temp\n",
    "\n",
    "    print(\"w2vec finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 22446/22446 [00:01<00:00, 13036.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if run_fastText:\n",
    "    print(\"fasttext starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_fasttextvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(fasttext.wv[i[0]])\n",
    "                except:\n",
    "                    pass\n",
    "            if len(patient_temp) == 0: continue\n",
    "            new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    print(\"fasttext finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Combined - Word2Vec & FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 22446/22446 [00:06<00:00, 3315.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if run_combined:\n",
    "    print(\"combined starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_concatvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "            \n",
    "            patient_temp = []\n",
    "        #     if k != 6: continue\n",
    "            for i in v:\n",
    "                w2vec_temp = []\n",
    "                try:\n",
    "                    w2vec_temp = w2vec.wv[i[0]]\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            w2vec_temp = [0] * 100\n",
    "                        elif num == 1:\n",
    "                            w2vec_temp = temp\n",
    "                        else:\n",
    "                            avg = np.array(avg)\n",
    "                            w2vec_temp = avg.mean(axis=0)\n",
    "                    else:\n",
    "                        w2vec_temp = [0] * 100\n",
    "\n",
    "                try:\n",
    "                    fasttemp = fasttext.wv[i[0]]\n",
    "                except:\n",
    "                    fasttemp = [0] * 100\n",
    "                    \n",
    "                appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "                patient_temp.append(appended)\n",
    "                \n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(\"combined finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BlueBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_blueBERT:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "    model = AutoModel.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "\n",
    "    batch_size = 128\n",
    "    patient_embeddings = []\n",
    "    \n",
    "    print(\"BlueBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_bluebert = {}\n",
    "    \n",
    "        for k, v in tqdm(data.items()):\n",
    "            # Batch input texts\n",
    "            input_texts = [i[0] for i in v]\n",
    "            encoded_inputs = tokenizer.batch_encode_plus(input_texts,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         return_tensors='pt',\n",
    "                                                         padding=True,\n",
    "                                                         truncation=True)\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_inputs).last_hidden_state\n",
    "            \n",
    "            # Extract [CLS] embeddings for each input text\n",
    "            embeddings = output[:, 0, :].tolist()\n",
    "            \n",
    "            if len(embeddings) == 0: \n",
    "                continue\n",
    "            new_bluebert[k] = embeddings\n",
    "        \n",
    "    # Convert values to NumPy arrays and store back in dictionary\n",
    "    for key in new_bluebert:\n",
    "        new_bluebert[key] = np.array(new_bluebert[key])\n",
    "    \n",
    "    print(\"BlueBERT finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_clinicalBERT:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "    model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "    batch_size = 128\n",
    "    patient_embeddings = []\n",
    "\n",
    "    print(\"ClinicalBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_clinicalbert = {}\n",
    "    \n",
    "        for k, v in tqdm(data.items()):\n",
    "            # Batch input texts\n",
    "            input_texts = [i[0] for i in v]\n",
    "            encoded_inputs = tokenizer.batch_encode_plus(input_texts,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         return_tensors='pt',\n",
    "                                                         padding=True,\n",
    "                                                         truncation=True)\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_inputs).last_hidden_state\n",
    "\n",
    "            \n",
    "            # Extract [CLS] embeddings for each input text\n",
    "            embeddings = output[:, 0, :].tolist()\n",
    "            \n",
    "            if len(embeddings) == 0: \n",
    "                continue\n",
    "            new_clinicalbert[k] = embeddings\n",
    "        \n",
    "    # Convert values to NumPy arrays and store back in dictionary\n",
    "    for key in new_clinicalbert:\n",
    "        new_clinicalbert[key] = np.array(new_clinicalbert[key])\n",
    "    \n",
    "    print(\"ClinicalBERT finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_word2vec:     pd.to_pickle(new_word2vec,     \"data/new_ner_word2vec_dict.pkl\")\n",
    "if run_fastText:     pd.to_pickle(new_fasttextvec,  \"data/new_ner_fasttext_dict.pkl\")\n",
    "if run_combined:     pd.to_pickle(new_concatvec,    \"data/new_ner_combined_dict.pkl\")\n",
    "if run_blueBERT:     pd.to_pickle(new_bluebert,     \"data/new_ner_bluebert_dict.pkl\")\n",
    "if run_clinicalBERT: pd.to_pickle(new_clinicalbert, \"data/new_ner_clinicalbert_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_word2vec loaded\n",
      "new_fasttextvec loaded\n",
      "new_concatvec loaded\n"
     ]
    }
   ],
   "source": [
    "new_word2vec     = pd.read_pickle(\"data/new_ner_word2vec_dict.pkl\")\n",
    "print(\"new_word2vec loaded\")\n",
    "new_fasttextvec  = pd.read_pickle(\"data/new_ner_fasttext_dict.pkl\")\n",
    "print(\"new_fasttextvec loaded\")\n",
    "new_concatvec    = pd.read_pickle(\"data/new_ner_combined_dict.pkl\")\n",
    "print(\"new_concatvec loaded\")\n",
    "# new_bluebertvec  = pd.read_pickle(\"data/new_ner_bluebert_dict.pkl\")\n",
    "# print(\"new_bluebert loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths before: 22203, 22025, 22446\n",
      "Lengths after:  22025, 22025, 22025\n"
     ]
    }
   ],
   "source": [
    "if run_word2vec and run_fastText and run_combined:\n",
    "\n",
    "    new_fasttextvec_keys  = set(new_fasttextvec.keys())\n",
    "    new_word2vec_keys     = set(new_word2vec.keys())\n",
    "    new_concatvec_keys    = set(new_concatvec.keys())\n",
    "    intersection_keys     = new_fasttextvec_keys.intersection(new_word2vec_keys).intersection(new_concatvec_keys)\n",
    "\n",
    "    print(\"Lengths before: {}, {}, {}\".format(len(new_word2vec), \n",
    "                                                  len(new_fasttextvec), \n",
    "                                                  len(new_concatvec)))\n",
    "\n",
    "    for i in new_fasttextvec_keys - intersection_keys:\n",
    "        del new_fasttextvec[i]\n",
    "    for i in new_word2vec_keys - intersection_keys:\n",
    "        del new_word2vec[i]\n",
    "    for i in new_concatvec_keys - intersection_keys:\n",
    "        del new_concatvec[i]\n",
    "\n",
    "    print(\"Lengths after:  {}, {}, {}\".format(len(new_word2vec), \n",
    "                                              len(new_fasttextvec), \n",
    "                                              len(new_concatvec)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_word2vec saved\n",
      "new_fasttextvec saved\n",
      "new_concatvec saved\n",
      "done with writing the files\n"
     ]
    }
   ],
   "source": [
    "pd.to_pickle(new_word2vec,    \"data/new_ner_word2vec_limited_dict.pkl\")\n",
    "print(\"new_word2vec saved\")\n",
    "pd.to_pickle(new_fasttextvec, \"data/new_ner_fasttext_limited_dict.pkl\")\n",
    "print(\"new_fasttextvec saved\")\n",
    "pd.to_pickle(new_concatvec,   \"data/new_ner_combined_limited_dict.pkl\")\n",
    "print(\"new_concatvec saved\")\n",
    "\n",
    "print(\"done with writing the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
